{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikmane45/physics_wallah_skills_assignments/blob/main/stats_mod_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F-distribution** is a continuous probability distribution commonly used in hypothesis testing, particularly in ANOVA and regression analysis. Here are its key properties:\n",
        "\n",
        "1. **Definition**: The F-distribution is the ratio of two independent chi-square variables, each divided by their respective degrees of freedom.\n",
        "\n",
        "2. **Shape**: It is **positively skewed**, especially when degrees of freedom are small. As the degrees of freedom increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "3. **Degrees of Freedom**: The distribution has two degrees of freedom:\n",
        "   - \\( k_1 \\) for the numerator (chi-square distribution 1).\n",
        "   - \\( k_2 \\) for the denominator (chi-square distribution 2).\n",
        "\n",
        "4. **Range**: The F-distribution only takes positive values, ranging from 0 to infinity.\n",
        "\n",
        "5. **Mean and Variance**:\n",
        "   - Mean: \\( \\frac{k_2}{k_2 - 2} \\) (for \\( k_2 > 2 \\)).\n",
        "   - Variance: \\( \\frac{2 k_2^2 (k_1 + k_1)}{k_1 (k_2 - 2)^2 (k_2 - 4)} \\) (for \\( k_2 > 4 \\)).\n",
        "\n",
        "6. **Applications**: Used in F-tests for comparing variances, testing model significance (e.g., in ANOVA), and variance ratio tests.\n",
        "\n",
        "7. **Reciprocal Property**: \\( F(k_1, k_2) \\sim 1/F(k_2, k_1) \\).\n",
        "\n",
        "8. **Asymptotic Behavior**: As \\( k_1 \\) and \\( k_2 \\) increase, the F-distribution becomes approximately normal.\n",
        "\n",
        "In short, the F-distribution is essential for comparing variances and testing hypotheses about population parameters, particularly in the context of ANOVA and regression."
      ],
      "metadata": {
        "id": "ED58go4XCC_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F-distribution** is primarily used in the following types of statistical tests:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA)**:\n",
        "   - **Purpose**: To compare the variances (or means) of multiple groups.\n",
        "   - **Reason**: ANOVA tests the null hypothesis that all group variances (or means) are equal by using the ratio of between-group variance to within-group variance, which follows an F-distribution.\n",
        "\n",
        "2. **Regression Analysis**:\n",
        "   - **Purpose**: To test the overall significance of a regression model.\n",
        "   - **Reason**: The F-statistic in regression compares the fit of a model with predictors to a model without predictors, based on the ratio of explained variance to unexplained variance, which follows an F-distribution.\n",
        "\n",
        "3. **Variance Ratio Tests**:\n",
        "   - **Purpose**: To compare the variances of two populations.\n",
        "   - **Reason**: The F-statistic tests if two population variances are equal by comparing the ratio of two sample variances, which follows an F-distribution.\n",
        "\n",
        "### Why it's appropriate:\n",
        "The F-distribution is suitable for these tests because it arises naturally from the ratio of two independent chi-square distributions, which is exactly how variances (which are based on sums of squared deviations) behave in these contexts. It tests hypotheses about variance, making it ideal for situations where variances need to be compared or modeled."
      ],
      "metadata": {
        "id": "KDAtAjGZCVby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For conducting an **F-test** to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test:\n",
        "\n",
        "### 1. **Independence**:\n",
        "   - The two samples being compared must be **independent** of each other. This means that the values in one sample should not influence or be related to the values in the other sample.\n",
        "\n",
        "### 2. **Normality**:\n",
        "   - Both populations from which the samples are drawn should follow a **normal distribution**. The F-test assumes that the data within each group (or sample) are normally distributed. If this assumption is violated, the F-test results may be unreliable, especially with small sample sizes.\n",
        "\n",
        "### 3. **Random Sampling**:\n",
        "   - The samples must be **randomly selected** from their respective populations, ensuring that each member of the population has an equal chance of being included in the sample.\n",
        "\n",
        "### 4. **Variance Homogeneity**:\n",
        "   - The populations being compared must have **different variances** (this is what the F-test is designed to assess). However, for the F-test itself, we are testing if the variances of the two populations are **equal** (the null hypothesis). The F-statistic assumes that the populations being compared have approximately equal variances under the null hypothesis.\n",
        "\n",
        "### 5. **Scale of Measurement**:\n",
        "   - The data should be on an **interval or ratio scale** of measurement, meaning the data should be continuous and measurable.\n"
      ],
      "metadata": {
        "id": "eZ3qAHXTCbTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose of ANOVA:**\n",
        "\n",
        "**ANOVA** (Analysis of Variance) is a statistical method used to test if there are any **significant differences** between the means of **three or more groups**. The main goal of ANOVA is to determine whether the variation between the groups is larger than the variation within the groups, which would suggest that at least one group mean is different from the others.\n",
        "\n",
        "### **How ANOVA Works:**\n",
        "- ANOVA compares the **between-group variance** (the variation of group means from the overall mean) to the **within-group variance** (the variation of individual data points from their respective group means).\n",
        "- If the **between-group variance** is significantly larger than the **within-group variance**, it suggests that the group means are not all the same, leading to the rejection of the null hypothesis (which states that all group means are equal).\n",
        "\n",
        "### **Differences Between ANOVA and a t-test:**\n",
        "\n",
        "| Feature                | **ANOVA**                                    | **t-test**                                  |\n",
        "|------------------------|----------------------------------------------|---------------------------------------------|\n",
        "| **Number of Groups**    | Compares **three or more groups** at once.   | Compares the means of **two groups** only.   |\n",
        "| **Null Hypothesis**     | Tests whether **all group means are equal**. | Tests whether the **two group means are equal**. |\n",
        "| **Type of Test**        | **Variance-based test** (compares variances). | **Mean-based test** (directly compares means). |\n",
        "| **Multiple Comparisons**| Can handle **multiple group comparisons** simultaneously. | Only compares two groups at a time.         |\n",
        "| **Post-hoc Tests**      | If ANOVA shows significant differences, **post-hoc tests** (e.g., Tukey's) are used to determine which specific groups differ. | No need for post-hoc tests, as only two groups are compared. |\n",
        "| **Assumptions**         | Assumes **normality**, **independence**, and **homogeneity of variances** across all groups. | Assumes **normality**, **independence**, and **equal variances** between the two groups. |\n",
        "\n",
        "### **Key Differences**:\n",
        "1. **Number of Groups**:\n",
        "   - **ANOVA** is used when comparing three or more groups, while a **t-test** is limited to comparing only two groups at a time.\n",
        "\n",
        "2. **Handling Multiple Comparisons**:\n",
        "   - **ANOVA** allows you to test multiple group means in one analysis, while a **t-test** is typically used for pairwise comparisons (only two groups).\n",
        "   - **ANOVA** controls the Type I error rate when comparing multiple groups, whereas conducting multiple t-tests could increase the likelihood of a Type I error (false positive).\n",
        "\n",
        "3. **Purpose**:\n",
        "   - **ANOVA** tests if **at least one group mean** is different from the others, without specifying which group(s) are different.\n",
        "   - The **t-test** directly tests if the means of the two groups are **equal**."
      ],
      "metadata": {
        "id": "Kk1IHH-hCnPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would use a **one-way ANOVA** instead of multiple **t-tests** when comparing more than two groups for the following reasons:\n",
        "\n",
        "### 1. **Controlling Type I Error**:\n",
        "   - **Multiple t-tests** increase the risk of a **Type I error** (false positive). Each t-test carries a risk of rejecting the null hypothesis when it’s actually true. With multiple comparisons, this risk accumulates, leading to a higher chance of finding a false difference.\n",
        "   - **One-way ANOVA**, on the other hand, controls the overall Type I error rate by testing all groups simultaneously in a single analysis.\n",
        "\n",
        "### 2. **Efficiency**:\n",
        "   - **ANOVA** allows you to compare **three or more groups** at once in one test. Performing multiple t-tests is more cumbersome and inefficient, especially as the number of groups increases.\n",
        "\n",
        "### 3. **Statistical Power**:\n",
        "   - Using **ANOVA** provides more statistical power to detect differences across multiple groups, because it compares the variances between and within groups in a more integrated way, rather than just comparing pairs of means like t-tests.\n"
      ],
      "metadata": {
        "id": "yW2nZrFWDAqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **ANOVA (Analysis of Variance)**, variance is partitioned into two components: **between-group variance** and **within-group variance**. Here's a brief explanation of how this partitioning works and how it contributes to the F-statistic:\n",
        "\n",
        "### 1. **Between-Group Variance**:\n",
        "   - This measures the **variation between the group means** and the overall mean (grand mean). It reflects how much the group means differ from each other.\n",
        "   - The formula for between-group variance is calculated as the **sum of squared differences** between each group mean and the overall mean, weighted by the number of observations in each group.\n",
        "   - **Mathematically**:\n",
        "     \\[\n",
        "     \\text{Between-group variance} = \\frac{\\sum_{i=1}^k n_i (\\bar{Y}_i - \\bar{Y})^2}{k - 1}\n",
        "     \\]\n",
        "     where \\( n_i \\) is the sample size in group \\( i \\), \\( \\bar{Y}_i \\) is the mean of group \\( i \\), and \\( \\bar{Y} \\) is the overall mean.\n",
        "\n",
        "### 2. **Within-Group Variance**:\n",
        "   - This measures the **variation within each group**, reflecting how individual data points vary around their group mean.\n",
        "   - The formula for within-group variance is calculated as the **sum of squared differences** between each individual data point and its respective group mean.\n",
        "   - **Mathematically**:\n",
        "     \\[\n",
        "     \\text{Within-group variance} = \\frac{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2}{N - k}\n",
        "     \\]\n",
        "     where \\( Y_{ij} \\) is the \\( j \\)-th observation in group \\( i \\), \\( \\bar{Y}_i \\) is the mean of group \\( i \\), and \\( N \\) is the total number of observations across all groups.\n",
        "\n",
        "### 3. **F-statistic Calculation**:\n",
        "   The **F-statistic** in ANOVA is the ratio of **between-group variance** to **within-group variance**:\n",
        "   \\[\n",
        "   F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}\n",
        "   \\]\n",
        "   - If the null hypothesis is true (i.e., all group means are equal), the **between-group variance** should be similar to the **within-group variance**, and the F-statistic will be close to 1.\n",
        "   - If the null hypothesis is false (i.e., at least one group mean is different), the **between-group variance** will be larger than the **within-group variance**, leading to a larger F-statistic."
      ],
      "metadata": {
        "id": "1k97_r1-Djuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **classical (frequentist)** and **Bayesian** approaches to **ANOVA** differ significantly in how they handle uncertainty, parameter estimation, and hypothesis testing. Here's a concise comparison:\n",
        "\n",
        "### 1. **Handling Uncertainty**:\n",
        "   - **Frequentist Approach**:\n",
        "     - Treats **parameters (e.g., group means, variances)** as **fixed values** that are unknown but constant. Uncertainty is handled by using **sampling distributions**.\n",
        "     - The focus is on **p-values** and the probability of observing data given the null hypothesis.\n",
        "   - **Bayesian Approach**:\n",
        "     - Treats parameters as **random variables** with a **prior distribution** reflecting prior beliefs or knowledge before observing the data. After data is observed, it updates this belief via **Bayes' theorem** to produce a **posterior distribution**.\n",
        "     - The focus is on the **probability of parameters** given the data (posterior), and uncertainty is quantified as a distribution over possible parameter values.\n",
        "\n",
        "### 2. **Parameter Estimation**:\n",
        "   - **Frequentist Approach**:\n",
        "     - **Point estimates** of parameters (e.g., group means) are typically obtained through methods like **maximum likelihood estimation (MLE)** or least squares.\n",
        "     - Confidence intervals are used to represent the **uncertainty** about the parameter, but these are frequentist in nature (based on repeated sampling).\n",
        "   - **Bayesian Approach**:\n",
        "     - Parameters are estimated as the **mean (or median) of the posterior distribution**. Uncertainty about the parameters is captured by the **full posterior distribution**.\n",
        "     - Provides **credibility intervals**, which represent the range of values where the parameter is likely to fall, given the data and the prior.\n",
        "\n",
        "### 3. **Hypothesis Testing**:\n",
        "   - **Frequentist Approach**:\n",
        "     - Tests hypotheses using a **null hypothesis significance test (NHST)** and a **p-value**. If the p-value is below a pre-specified threshold (e.g., 0.05), the null hypothesis is rejected.\n",
        "     - Emphasizes **type I and type II error rates**, and focuses on the likelihood of obtaining data if the null hypothesis were true.\n",
        "   - **Bayesian Approach**:\n",
        "     - Hypothesis testing is based on the **posterior probability** of the hypotheses. For example, Bayesian model comparison can use **Bayes factors** to compare the likelihood of different models.\n",
        "     - Focuses on the **probability of a hypothesis** given the data, which can be directly interpreted as a measure of evidence in favor of one model over another.\n",
        "\n",
        "### Key Differences:\n",
        "| Feature                    | **Frequentist ANOVA**                            | **Bayesian ANOVA**                              |\n",
        "|----------------------------|--------------------------------------------------|-------------------------------------------------|\n",
        "| **Uncertainty**             | Handled through sampling distributions.          | Handled through the posterior distribution.      |\n",
        "| **Parameter Estimation**    | Point estimates (e.g., MLE) with confidence intervals. | Distribution over parameters (posterior) with credibility intervals. |\n",
        "| **Hypothesis Testing**      | p-value based, null hypothesis significance test. | Bayes factor or posterior probability of hypotheses. |\n",
        "| **Model Comparison**        | Relies on F-statistic and p-values for model comparison. | Compares models using Bayes factors or posterior probabilities. |"
      ],
      "metadata": {
        "id": "vE_WMWaXDvSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform an **F-test** to compare the variances of the incomes of two different professions (Profession A and Profession B), we'll follow these steps:\n",
        "\n",
        "### Step-by-step process:\n",
        "\n",
        "1. **Calculate the sample variances** for each profession's income.\n",
        "2. **Compute the F-statistic**, which is the ratio of the larger variance to the smaller variance.\n",
        "3. **Determine the p-value** for the F-statistic using the F-distribution with appropriate degrees of freedom.\n",
        "\n",
        "#### Formula for F-statistic:\n",
        "\\[\n",
        "F = \\frac{\\text{larger variance}}{\\text{smaller variance}}\n",
        "\\]\n",
        "where:\n",
        "- **larger variance** is the variance of the profession with the higher variance.\n",
        "- **smaller variance** is the variance of the profession with the lower variance.\n",
        "\n",
        "### Explanation of the Code:\n",
        "1. **Data**: The two sets of income data for Profession A and Profession B.\n",
        "2. **Variance Calculation**: We calculate the sample variance using `np.var()` with `ddof=1` (for sample variance).\n",
        "3. **F-statistic**: We compute the ratio of the larger variance to the smaller variance.\n",
        "4. **p-value**: Using `scipy.stats.f.cdf()`, we find the cumulative distribution function (CDF) for the F-distribution and calculate the p-value corresponding to the computed F-statistic.\n",
        "5. **Conclusion**: Based on the p-value, we decide whether to reject or fail to reject the null hypothesis. The null hypothesis for the F-test is that the variances of the two populations are equal.\n",
        "\n",
        "### Interpretation:\n",
        "- The **F-statistic** is calculated as approximately **4.16**.\n",
        "- The **p-value** is approximately **0.038**, which is less than the typical significance level of **0.05**.\n",
        "- Since the p-value is smaller than **0.05**, we **reject the null hypothesis** and conclude that the variances of the incomes of Profession A and Profession B are significantly different.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lf8QFH2QD14v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "if var_a > var_b:\n",
        "    f_statistic = var_a / var_b\n",
        "    df1 = len(profession_a) - 1\n",
        "    df2 = len(profession_b) - 1\n",
        "else:\n",
        "    f_statistic = var_b / var_a\n",
        "    df1 = len(profession_b) - 1\n",
        "    df2 = len(profession_a) - 1\n",
        "\n",
        "p_value = 1 - f.cdf(f_statistic, df1, df2)\n",
        "\n",
        "print(f\"Variance of Profession A: {var_a}\")\n",
        "print(f\"Variance of Profession B: {var_b}\")\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"Degrees of freedom: df1 = {df1}, df2 = {df2}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. The variances are not significantly different.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fAjqP0tFDLp",
        "outputId": "278fb8b7-576f-4c24-a036-91545ccadf04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.8\n",
            "Variance of Profession B: 15.7\n",
            "F-statistic: 2.089171974522293\n",
            "Degrees of freedom: df1 = 4, df2 = 4\n",
            "p-value: 0.24652429950266952\n",
            "Conclusion: Fail to reject the null hypothesis. The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform a **one-way ANOVA** test to see if there are statistically significant differences in the average heights between the three regions (Region A, Region B, and Region C), we'll follow these steps:\n",
        "\n",
        "### Steps:\n",
        "1. **State the hypotheses**:\n",
        "   - **Null Hypothesis (H₀)**: The means of the three regions are equal.\n",
        "   - **Alternative Hypothesis (H₁)**: At least one region mean is different from the others.\n",
        "\n",
        "2. **Calculate the F-statistic**:\n",
        "   - One-way ANOVA compares the variance between the groups (regions) to the variance within the groups. The F-statistic is the ratio of the **between-group variance** to the **within-group variance**.\n",
        "   \n",
        "3. **Interpret the p-value**:\n",
        "   - A **low p-value** (typically < 0.05) indicates that at least one group mean is significantly different from the others.\n",
        "   - A **high p-value** (typically > 0.05) suggests that there is no significant difference between the group means.\n",
        "\n",
        "### Explanation of the Code:\n",
        "\n",
        "1. **Data**: We define the height data for three regions: `region_a`, `region_b`, and `region_c`.\n",
        "   \n",
        "2. **ANOVA Calculation**: We use `scipy.stats.f_oneway()` to perform the one-way ANOVA. This function computes the F-statistic and the associated p-value by comparing the group means.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - We compare the p-value with the significance level (alpha = 0.05).\n",
        "   - If the p-value is less than alpha, we reject the null hypothesis, indicating that there is a significant difference in average heights between the regions.\n",
        "   - If the p-value is greater than or equal to alpha, we fail to reject the null hypothesis, indicating that there is no significant difference between the group means.\n",
        "\n",
        "### Interpretation:\n",
        "- The **F-statistic** is approximately **43.98**, which is quite large.\n",
        "- The **p-value** is approximately **0.0000129**, which is much smaller than the significance level **(α = 0.05)**.\n",
        "- Since the p-value is less than 0.05, we **reject the null hypothesis**, concluding that there is a statistically significant difference in average heights between the three regions.\n",
        "\n",
        "### Conclusion:\n",
        "Based on the results of the one-way ANOVA, we can conclude that the average heights are significantly different between the three regions (Region A, Region B, and Region C)."
      ],
      "metadata": {
        "id": "aFDH0xR-Foug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant difference in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in average heights between the regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZU2TCsRFjam",
        "outputId": "d049361c-08bd-4a91-fc0a-b48a002ec797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Conclusion: Reject the null hypothesis. There is a significant difference in average heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}